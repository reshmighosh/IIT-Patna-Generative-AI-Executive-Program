{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b75760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Packages\n",
    "# Run this cell first to install all necessary dependencies\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    \"\"\"Install a package using pip\"\"\"\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"✅ Successfully installed {package}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"❌ Failed to install {package}: {e}\")\n",
    "\n",
    "# List of required packages\n",
    "packages = [\n",
    "    \"torch>=1.9.0\",\n",
    "    \"transformers>=4.20.0\", \n",
    "    \"huggingface_hub>=0.10.0\",\n",
    "    \"tokenizers>=0.12.0\",\n",
    "    \"datasets\",\n",
    "    \"accelerate\",\n",
    "]\n",
    "\n",
    "print(\"Installing required packages for Hugging Face model demonstration...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "for package in packages:\n",
    "    install_package(package)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📦 Package installation complete!\")\n",
    "print(\"💡 For GPU support, you may also want to install:\")\n",
    "print(\"   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42ae2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Model Demonstration with non Generative AI Language Models\n",
    "## Loading Models with Authentication and Performing NLP Tasks\n",
    "\n",
    "#This notebook demonstrates:\n",
    "#1. Setting up Hugging Face authentication\n",
    "#2. Loading models for summarization and extractive QA\n",
    "#3. Using simple and complex prompts\n",
    "#4. Comparing different model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f308540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    AutoModelForQuestionAnswering,\n",
    "    pipeline\n",
    ")\n",
    "from huggingface_hub import login\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18219d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication Setup\n",
    "# Option 1: Set your Hugging Face token as an environment variable\n",
    "# os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"your_token_here\"\n",
    "\n",
    "# Option 2: Use huggingface_hub login (recommended for interactive use)\n",
    "# Uncomment the line below and run to authenticate interactively\n",
    "# login()\n",
    "\n",
    "# Option 3: Login programmatically (for automation)\n",
    "# login(token=\"your_token_here\")\n",
    "\n",
    "# For this demo, we'll check if authentication is available\n",
    "try:\n",
    "    from huggingface_hub import HfApi\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Authenticated as: {user_info['name']}\")\n",
    "    authenticated = True\n",
    "except Exception as e:\n",
    "    print(\"Not authenticated - will use public models only\")\n",
    "    print(\"To authenticate, uncomment one of the login options above\")\n",
    "    authenticated = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb092fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Summarization Models\n",
    "print(\"Loading summarization models...\")\n",
    "\n",
    "# Model 1: BART for summarization (good for general purpose)\n",
    "summarizer_bart = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\",\n",
    "    tokenizer=\"facebook/bart-large-cnn\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Model 2: T5 for summarization (good for instruction-following)\n",
    "summarizer_t5 = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"t5-small\",  # Use t5-base or t5-large for better quality\n",
    "    tokenizer=\"t5-small\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"✓ Summarization models loaded successfully!\")\n",
    "print(f\"BART model device: {summarizer_bart.device}\")\n",
    "print(f\"T5 model device: {summarizer_t5.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a523f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Question Answering Models\n",
    "print(\"Loading question answering models...\")\n",
    "\n",
    "# Model 1: DistilBERT for QA (fast and efficient)\n",
    "qa_distilbert = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"distilbert-base-cased-distilled-squad\",\n",
    "    tokenizer=\"distilbert-base-cased-distilled-squad\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "# Model 2: RoBERTa for QA (higher accuracy)\n",
    "qa_roberta = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"deepset/roberta-base-squad2\",\n",
    "    tokenizer=\"deepset/roberta-base-squad2\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "print(\"✓ Question answering models loaded successfully!\")\n",
    "print(f\"DistilBERT QA device: {qa_distilbert.device}\")\n",
    "print(f\"RoBERTa QA device: {qa_roberta.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5843b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Texts for Demonstration\n",
    "\n",
    "# Simple text for basic testing\n",
    "simple_text = \"\"\"\n",
    "Artificial intelligence (AI) is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans. \n",
    "AI research has been highly successful in developing effective techniques for solving a wide range of problems, from game playing to medical diagnosis.\n",
    "\"\"\"\n",
    "\n",
    "# Complex text for advanced testing\n",
    "complex_text = \"\"\"\n",
    "The field of artificial intelligence has undergone remarkable transformations since its inception in the 1950s. \n",
    "Initially focused on symbolic reasoning and expert systems, AI research has evolved to encompass machine learning, \n",
    "deep learning, and neural networks. Modern AI systems leverage vast amounts of data and computational power to \n",
    "perform tasks that were once thought to require human-level intelligence.\n",
    "\n",
    "Large language models, such as GPT and BERT, have revolutionized natural language processing by demonstrating \n",
    "unprecedented capabilities in text generation, comprehension, and translation. These models are trained on \n",
    "massive datasets containing billions of words from diverse sources, enabling them to capture complex patterns \n",
    "in human language.\n",
    "\n",
    "The applications of AI span numerous domains, including healthcare, finance, transportation, and entertainment. \n",
    "In healthcare, AI systems assist in medical imaging analysis, drug discovery, and personalized treatment plans. \n",
    "Autonomous vehicles rely on computer vision and sensor fusion to navigate complex traffic scenarios. \n",
    "Recommendation systems power content discovery on streaming platforms and e-commerce websites.\n",
    "\n",
    "However, the rapid advancement of AI also raises important ethical considerations. Issues such as algorithmic bias, \n",
    "privacy concerns, job displacement, and the potential for misuse of AI technologies require careful attention \n",
    "from researchers, policymakers, and society as a whole. Ensuring that AI development proceeds in a responsible \n",
    "and beneficial manner remains one of the most significant challenges of our time.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sample texts loaded:\")\n",
    "print(f\"Simple text length: {len(simple_text)} characters\")\n",
    "print(f\"Complex text length: {len(complex_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97e615",
   "metadata": {},
   "source": [
    "## Summarization Tasks\n",
    "\n",
    "### Simple Prompts\n",
    "Let's start with basic summarization using different models and compare their outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Summarization with BART\n",
    "print(\"=== Simple Summarization with BART ===\")\n",
    "print(\"Original text:\")\n",
    "print(simple_text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Basic summarization\n",
    "bart_summary = summarizer_bart(simple_text, max_length=50, min_length=20, do_sample=False)\n",
    "print(\"BART Summary:\")\n",
    "print(bart_summary[0]['summary_text'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# T5 summarization (requires different prompt format)\n",
    "t5_input = \"summarize: \" + simple_text\n",
    "t5_summary = summarizer_t5(t5_input, max_length=60, min_length=20, do_sample=False)\n",
    "print(\"T5 Summary:\")\n",
    "print(t5_summary[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df99c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex Summarization with Different Styles\n",
    "print(\"=== Complex Text Summarization ===\")\n",
    "\n",
    "# Standard summarization\n",
    "print(\"1. Standard Summary (BART):\")\n",
    "bart_complex = summarizer_bart(complex_text, max_length=100, min_length=50, do_sample=False)\n",
    "print(bart_complex[0]['summary_text'])\n",
    "\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# Bullet point style summary with T5\n",
    "print(\"2. Bullet Point Style Summary (T5):\")\n",
    "bullet_prompt = \"summarize in bullet points: \" + complex_text\n",
    "t5_bullets = summarizer_t5(bullet_prompt, max_length=150, min_length=50, do_sample=False)\n",
    "print(t5_bullets[0]['generated_text'])\n",
    "\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# Executive summary style\n",
    "print(\"3. Executive Summary Style (T5):\")\n",
    "exec_prompt = \"write an executive summary: \" + complex_text\n",
    "t5_exec = summarizer_t5(exec_prompt, max_length=120, min_length=60, do_sample=False)\n",
    "print(t5_exec[0]['generated_text'])\n",
    "\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# Short social media style summary\n",
    "print(\"4. Social Media Style Summary (BART - very short):\")\n",
    "bart_short = summarizer_bart(complex_text, max_length=30, min_length=15, do_sample=False)\n",
    "print(bart_short[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6da95d",
   "metadata": {},
   "source": [
    "## Extractive Question Answering Tasks\n",
    "\n",
    "### Simple Questions\n",
    "Let's test the QA models with straightforward questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c394ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Question Answering\n",
    "def ask_question(question, context, model_name, qa_pipeline):\n",
    "    \"\"\"Helper function to ask questions and display results\"\"\"\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {result['answer']}\")\n",
    "    print(f\"Confidence: {result['score']:.4f}\")\n",
    "    print(f\"Context span: ...{context[result['start']:result['end']]}...\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Simple questions on simple text\n",
    "simple_questions = [\n",
    "    \"What is artificial intelligence?\",\n",
    "    \"What has AI research been successful in?\",\n",
    "    \"What type of intelligence do humans display?\"\n",
    "]\n",
    "\n",
    "print(\"=== Simple QA on Simple Text ===\\n\")\n",
    "\n",
    "for question in simple_questions:\n",
    "    ask_question(question, simple_text, \"DistilBERT\", qa_distilbert)\n",
    "    ask_question(question, simple_text, \"RoBERTa\", qa_roberta)\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9597c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex Question Answering on Complex Text\n",
    "complex_questions = [\n",
    "    \"When was the field of artificial intelligence founded?\",\n",
    "    \"What are some examples of large language models mentioned?\",\n",
    "    \"In which domains are AI applications mentioned?\",\n",
    "    \"What ethical considerations are raised about AI?\",\n",
    "    \"What techniques were AI initially focused on?\",\n",
    "    \"How do autonomous vehicles use AI?\",\n",
    "    \"What is required for responsible AI development?\"\n",
    "]\n",
    "\n",
    "print(\"=== Complex QA on Complex Text ===\\n\")\n",
    "\n",
    "for i, question in enumerate(complex_questions, 1):\n",
    "    print(f\"Question {i}:\")\n",
    "    \n",
    "    # Get answers from both models\n",
    "    distilbert_result = qa_distilbert(question=question, context=complex_text)\n",
    "    roberta_result = qa_roberta(question=question, context=complex_text)\n",
    "    \n",
    "    print(f\"Q: {question}\")\n",
    "    print(f\"DistilBERT: {distilbert_result['answer']} (confidence: {distilbert_result['score']:.3f})\")\n",
    "    print(f\"RoBERTa: {roberta_result['answer']} (confidence: {roberta_result['score']:.3f})\")\n",
    "    \n",
    "    # Highlight if there's a significant confidence difference\n",
    "    conf_diff = abs(distilbert_result['score'] - roberta_result['score'])\n",
    "    if conf_diff > 0.2:\n",
    "        print(f\"⚠️  Large confidence difference: {conf_diff:.3f}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a18bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced QA Techniques\n",
    "print(\"=== Advanced QA Techniques ===\\n\")\n",
    "\n",
    "# 1. Multi-hop reasoning (questions requiring combining information)\n",
    "print(\"1. Multi-hop Reasoning:\")\n",
    "multi_hop_q = \"What challenges exist for AI applications in healthcare and transportation?\"\n",
    "result = qa_roberta(question=multi_hop_q, context=complex_text)\n",
    "print(f\"Q: {multi_hop_q}\")\n",
    "print(f\"A: {result['answer']} (confidence: {result['score']:.3f})\")\n",
    "print()\n",
    "\n",
    "# 2. Negation handling\n",
    "print(\"2. Negation Handling:\")\n",
    "negation_q = \"What were AI systems initially NOT focused on?\"\n",
    "result = qa_roberta(question=negation_q, context=complex_text)\n",
    "print(f\"Q: {negation_q}\")\n",
    "print(f\"A: {result['answer']} (confidence: {result['score']:.3f})\")\n",
    "print()\n",
    "\n",
    "# 3. Comparative questions\n",
    "print(\"3. Comparative Questions:\")\n",
    "comparative_q = \"How do modern AI systems differ from early AI research?\"\n",
    "result = qa_roberta(question=comparative_q, context=complex_text)\n",
    "print(f\"Q: {comparative_q}\")\n",
    "print(f\"A: {result['answer']} (confidence: {result['score']:.3f})\")\n",
    "print()\n",
    "\n",
    "# 4. Questions with no answer (should return low confidence or empty)\n",
    "print(\"4. Unanswerable Questions:\")\n",
    "no_answer_q = \"What programming language is best for AI development?\"\n",
    "result = qa_roberta(question=no_answer_q, context=complex_text)\n",
    "print(f\"Q: {no_answer_q}\")\n",
    "print(f\"A: {result['answer']} (confidence: {result['score']:.3f})\")\n",
    "if result['score'] < 0.1:\n",
    "    print(\"⚠️  Low confidence - question likely unanswerable from context\")\n",
    "print()\n",
    "\n",
    "# 5. Batch processing multiple questions\n",
    "print(\"5. Batch Processing:\")\n",
    "batch_questions = [\n",
    "    \"What enables large language models to capture complex patterns?\",\n",
    "    \"Where do recommendation systems work?\",\n",
    "    \"What requires careful attention from researchers?\"\n",
    "]\n",
    "\n",
    "print(\"Processing multiple questions in batch...\")\n",
    "for q in batch_questions:\n",
    "    result = qa_distilbert(question=q, context=complex_text)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"A: {result['answer'][:100]}...\" if len(result['answer']) > 100 else f\"A: {result['answer']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196637aa",
   "metadata": {},
   "source": [
    "## Model Comparison and Performance Tips\n",
    "\n",
    "### Key Differences Between Models\n",
    "- **BART vs T5 for Summarization**: BART is pre-trained specifically for text generation tasks, while T5 uses a text-to-text approach\n",
    "- **DistilBERT vs RoBERTa for QA**: DistilBERT is faster and smaller, RoBERTa typically provides higher accuracy\n",
    "- **Authentication Benefits**: Authenticated access provides higher rate limits and access to private/gated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0db197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Comparison and Optimization Tips\n",
    "import time\n",
    "\n",
    "def benchmark_models():\n",
    "    \"\"\"Benchmark the performance of different models\"\"\"\n",
    "    print(\"=== Model Performance Benchmark ===\\n\")\n",
    "    \n",
    "    test_text = complex_text[:500]  # Use first 500 chars for consistent testing\n",
    "    test_question = \"What is artificial intelligence?\"\n",
    "    \n",
    "    # Summarization benchmark\n",
    "    print(\"Summarization Performance:\")\n",
    "    \n",
    "    # BART timing\n",
    "    start_time = time.time()\n",
    "    bart_result = summarizer_bart(test_text, max_length=50, min_length=20)\n",
    "    bart_time = time.time() - start_time\n",
    "    print(f\"BART: {bart_time:.3f}s - {bart_result[0]['summary_text'][:50]}...\")\n",
    "    \n",
    "    # T5 timing\n",
    "    start_time = time.time()\n",
    "    t5_result = summarizer_t5(\"summarize: \" + test_text, max_length=50, min_length=20)\n",
    "    t5_time = time.time() - start_time\n",
    "    print(f\"T5: {t5_time:.3f}s - {t5_result[0]['generated_text'][:50]}...\")\n",
    "    \n",
    "    print(f\"\\nSummarization Speed Comparison: BART is {t5_time/bart_time:.1f}x {'faster' if bart_time < t5_time else 'slower'} than T5\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # QA benchmark\n",
    "    print(\"Question Answering Performance:\")\n",
    "    \n",
    "    # DistilBERT timing\n",
    "    start_time = time.time()\n",
    "    distilbert_result = qa_distilbert(question=test_question, context=test_text)\n",
    "    distilbert_time = time.time() - start_time\n",
    "    print(f\"DistilBERT: {distilbert_time:.3f}s - {distilbert_result['answer'][:30]}... (conf: {distilbert_result['score']:.3f})\")\n",
    "    \n",
    "    # RoBERTa timing\n",
    "    start_time = time.time()\n",
    "    roberta_result = qa_roberta(question=test_question, context=test_text)\n",
    "    roberta_time = time.time() - start_time\n",
    "    print(f\"RoBERTa: {roberta_time:.3f}s - {roberta_result['answer'][:30]}... (conf: {roberta_result['score']:.3f})\")\n",
    "    \n",
    "    print(f\"\\nQA Speed Comparison: DistilBERT is {roberta_time/distilbert_time:.1f}x {'faster' if distilbert_time < roberta_time else 'slower'} than RoBERTa\")\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_models()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZATION TIPS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Use GPU when available (device=0) for faster inference\")\n",
    "print(\"2. Batch multiple inputs together for better throughput\")\n",
    "print(\"3. Consider model size vs. accuracy trade-offs:\")\n",
    "print(\"   - DistilBERT: 66M params, fastest\")\n",
    "print(\"   - BART-large: 400M params, good balance\")\n",
    "print(\"   - T5-base: 220M params, versatile\")\n",
    "print(\"4. Cache models locally to avoid re-downloading\")\n",
    "print(\"5. Use quantization for deployment (not shown here)\")\n",
    "print(\"6. Adjust max_length parameters based on your needs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d13a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation Requirements and Setup\n",
    "print(\"=== Required Packages ===\")\n",
    "print(\"To run this notebook, install the following packages:\")\n",
    "print()\n",
    "\n",
    "requirements = [\n",
    "    \"torch>=1.9.0\",\n",
    "    \"transformers>=4.20.0\", \n",
    "    \"huggingface_hub>=0.10.0\",\n",
    "    \"tokenizers>=0.12.0\",\n",
    "    \"datasets\",  # Optional, for additional datasets\n",
    "    \"accelerate\",  # For faster training/inference\n",
    "]\n",
    "\n",
    "for req in requirements:\n",
    "    print(f\"pip install {req}\")\n",
    "\n",
    "print(\"\\nFor GPU support (optional but recommended):\")\n",
    "print(\"pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"AUTHENTICATION SETUP:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. Create account at https://huggingface.co/\")\n",
    "print(\"2. Generate access token at https://huggingface.co/settings/tokens\")\n",
    "print(\"3. Set environment variable: export HUGGINGFACE_HUB_TOKEN='your_token'\")\n",
    "print(\"4. Or use: huggingface-cli login\")\n",
    "print(\"5. Or use: from huggingface_hub import login; login()\")\n",
    "\n",
    "print(\"\\n✅ Setup complete! You can now run all the cells above.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
